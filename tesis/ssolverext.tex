%!TEX root = tesis.tex

\section{Sobre la automatización}

En la presente sección nos proponemos señalar los aspectos importantes a tener
en cuenta relacionados con la automatización del funcionamiento de nuestra
herramienta. Además señalaremos las decisiones adoptadas por nosotros en cada
uno de los puntos relevantes.

\subsection{Consideraciones sobre la estrategia}
% - Noción de problema difícil vs problema fácil
% - Imposibilidad de determinar o siquiera estimar eso a priori
% - Enorme varianza entre los subproblemas de un problema

% - Cuándo declarar que un problema es "demasiado difícil" (para
% atacarlo sin partirlo)?
% - Cuáles de los solvings en curso abortar? Cuántos? Cuándo?

%         - Imposibilidad de determinar o siquiera estimar a priori (en base a
%                 criterios estáticos) la dificultad de un (sub)problema.

%         - No sabemos cuánto va a tardar.
%         - No sabemos si es fácil o difícil.

%         - Partir temprano algo fácil (una rama que con poco más se cerraba)
%           es un error que se paga caro, sobre todo si se repite recursivamente.

%         - Invertir mucho esfuerzo en algo difícil y encima no lograr cerrarlo
%           (sólo para terminar partiéndolo igual, y mucho después) es otro
%           error que se paga caro, sobre todo si se repite recursivamente.

%         - Invertir mucho esfuerzo en algo difícil hasta cerrarlo sin partirlo
%           no siempre es una buena idea (atenta contra el paralelismo,
%           sube el camino crítico, etc).

%         - No existe un valor de TO prefijado que sea adecuado para cualquier
%           problema raíz (y no queremos que el usuario tenga que proveer uno).

%         - Incluso para cierto problema raíz R, no existe un único valor de TO
%           que sea adecuado durante todo el transcurso de la corrida.

%                 - Cada vez que se parte un subproblema (sup. elección razonable de
%                   vars) se obtienen hijos más fáciles, pero

%                     - con mucha varianza en su distribución [tablita pamela8?]

%                     - con tan poca predecibilidad como dijimos antes respecto de
%                       la tasa (hijo_mas_caro/padre)

%                 - O sea que lo único que realmente sabemos es que "se van haciendo
%                   más fáciles" (pero no a qué velocidad, ni cuánto más rápido a lo
%                   largo de una rama vs. de otra, etc).

%         => Es necesario algún criterio dinámico / adaptativo / etc.

%         - Podría basarse en
%                 - aprendizaje sobre el problema (qué pasó hasta ahora otras veces, etc)
%                 - feedback loop observando métricas del comportamiento/estado del sistema


% - Cómo partir un problema demasiado difícil?

%         - Cuáles y cuántas variables levantar

%                 => Gran Temón en sat-solving, recontra determinante para la
%                   cant/dificultad/distribución de los subproblemas, etc
%                   VSIDS, etc.

%                 => Nivel de agresividad, fanout, potencial explosión;
%                         trade-off "muchos" vs "más fáciles" (ojalá!);
%                         hay que tener cuidado con el fanout recursivo.

%         - Filtrado de subproblemas triviales

%                 - Rara vez se da el peor caso 2^n totalmente denso
%                 - Muchas veces se puede lograr mucho menos!
%                 - Hay maneras muy baratas de filtrar los muy triviales
%                 - Hay maneras (no tan) baratas de filtrar los (no tan) triviales
%                 => Trade-off:
%                         - escaso filtrado =>
%                             overhead innecesario por proliferación de tareas fáciles evitables
%                         - exceso de esfuerzo en filtrado =>
%                             excesiva centralización de costo computacional que podría distribuirse


% - Ni bien alguien queda ocioso debe asignársele más trabajo.

% - Decisión importante: ¿cuál de todas las tareas pendientes es la "próxima"?
%   => afecta cómo se recorre el espacio de búsqueda
%   => termina afectando el scheduling (en presencia de feedback loops etc)

% - Además, minimizar costos de movimientos de tareas innecesarios
%   => si ese alguien ya tiene trabajo pendiente local, podría ser bueno
%      que se le asigne lo mejor posible dentro de lo local

Comenzaremos esta sección desglosando las principales cuestiones a
resolver por una estrategia completamente automática para nuestra herramienta.
Surge así la primer pregunta relevante que cualquier estrategia debe
responder: ¿Cuándo declarar que un (sub)problema es demasiado difícil para
atacarlo como tal? Es decir: ¿Cuándo se toma la decisión de partir un problema
en nuevos subproblemas?

\subsubsection{Dificultad de un problema}

En este punto es importante destacar el hecho de que la partición de un
problema en nuevos subproblemas se lleva a cabo con el objetivo de distribuir
las distintas porciones del espacio de búsqueda entre distintas unidades de
cómputo. Así, invertir demasiado tiempo en intentar obtener un resultado
secuencialmente para un problema difícil atenta prohibitivamente contra el
paralelismo. Por lo tanto la decisión de cuándo abortar un \solving en curso
se vuelve crucial.

Si un problema va a tomar demasiado tiempo no tiene sentido invertir tiempo de
\solving secuencial en intentar resolverlo y es conveniente partirlo
\emph{cuanto antes}. Por otra parte, partir un problema en un momento tal que
si hubiéramos esperado una fracción  pequeña de tiempo más el \w habría
arribado a un resultado para el problema implica que \emph{todo} el tiempo de
cómputo previamente invertido es desperdiciado. Estos dos errores se vuelven
catastróficos cuando se repiten en forma sistemática sobre los subproblemas a los que un problema da origen.

Subyace a toda esta cuestión la pregunta fundamental de cómo determinar que un
problema dado es fácil o difícil. Si tuviéramos algún criterio que nos
permitiese siquiera aproximar la dificultad de un problema dado, o el
porcentaje de avance durante una corrida secuencial de un \ssolver, podríamos
elaborar una estrategia que atienda razonablemente a las inquietudes
planteadas arriba. Lamentablemente no se conoce ninguna métrica que permita
establecer \apriori la dificultad de un problema. Si bien la complejidad de
los algoritmos de \ssolving se expresa en función de la cantidad de variables
de un problema (y es exponencial), la experiencia indica que existen problemas
con pocas variables sumamente difíciles y problemas con muchas variables
razonablemente sencillos.

\subsubsection{Cuándo partir: criterios estáticos vs. dinámicos}

Resulta claro entonces que una primera aproximación \emph{naive} a este
problema, como por ejemplo la elección de un \tout fijo a modo de criterio para
determinar cuándo un problema debe ser partido, resulta una mala alternativa.
La disparidad en la dificultad de los distintos problemas hace imposible
dar con un único valor prefijado de \tout que resulte adecuado para el
caso general. Otra posibilidad sería entonces que el usuario seleccionara
el \tout deseado para cada problema. Sin embargo, esto atenta sustancialmente
contra la usabilidad de la herramienta, ya que requiere asumir un \emph{expertise} por
parte del usuario y un conocimiento previo del problema a resolver que no son
razonables.

Además, a medida que partimos recursivamente un problema, los subproblemas
generados --si la elección de variables a levantar fue apropiada-- son
\emph{cada vez} más fáciles. Por lo tanto un valor de \tout que resulte
adecuado en cierto momento ya no lo será más adelante. Esto podría llevarnos a
elaborar un criterio un tanto más refinado que lo anterior. Por ejemplo,
podríamos optar por tener un valor de \tout para cada \emph{nivel} del
problema. Sin embargo, surge nuevamente el mismo inconveniente: cada vez
que un problema es partido, los subproblemas generados difieren notablemente
en su nivel de dificultad. Más aún, las distribuciones de dificultad entre
los hijos de dos subproblemas distintos de un mismo problema (a misma cantidad
de variables levantadas) pueden ser muy distintas. En consecuencia, este
refinamiento tampoco resulta adecuado.

En general, la ausencia de un criterio estático para determinar la dificultad
de un problema dado hace imposible la elección de un criterio de corte que
sea también estático. Por lo tanto se vuelve necesario que el criterio para
determinar que un problema debe ser partido sea dinámico. Es decir, que el
mismo se vaya adaptando durante la ejecución. Un criterio de estas
características tiene dos fuentes de información fundamentales. Por un lado el
estado general del sistema y su evolución (su historia). Esto puede incluir
métricas sobra la carga del sistema, cantidad de problemas pendientes, espacio
de almacenamiento disponible, etc. La segunda fuente de información surge de
las características del problema que sean observables a lo largo de la
ejecución. Ejemplos de esto son la cantidad de subproblemas producidos cada
vez que se partió un problema, estadísticas sobre los tiempos que demoraron
los subproblemas ya resueltos, etc. 

Cualquier estrategia automática razonable deberá entoces dar respuesta a la
problemática de decidir cuándo partir un problema teniendo en cuenta las
observaciones mencionadas. Una vez resuelto este dilema surge la segunda
pregunta fundamental a responder que es: ¿Cómo partir un problema? Que
esencialmente se traduce en: ¿Cuántas y qué variables levantar en cada ocasión?

\subsubsection{Cómo partir: criterios de selección de variables}

\newcommand{\vsids}{VSIDS\xspace}

La determinación de qué variables levantar en el momento en que se decide
partir un problema es uno de los grandes temas en el mundo del \ssolving. En
un sentido, este problema es equivalente al problema que enfrentan los
\ssolver secuenciales cuando deben tomar una decisión. Cuando un \ssolver
secuencial agotó las propagaciones de la última decisión tomada, debe tomar
una nueva decisión. Tomar una decisión implica en primera instancia
seleccionar una variable y luego seleccionar cuál de los dos posibles valores
de verdad asignar a esa variable. En nuestro caso, dado que al levantar una
variable los dos valores de verdad son considerados \emph{simultáneamente}, el
problema se reduce a elegir cuidadosamente la variable a levantar. Uno de los
métodos más difundidos para elegir la próxima variable en los \ssolvers
secuenciales, se basa en un criterio de actividad conocido como \vsids. Del
mismo modo que para las cláusulas --ver Sec.~\ref{sec:longactlbd}-- cada vez que
una variable \emph{participa} de un conflicto su actividad es incrementada.
Luego, cuando el \ssolver se encuentra en la situación de tener que elegir una
variable para continuar la búsqueda, elige aquella con mayor actividad.

La elección de las variables a ser levantadas es crucial. Como se ve en
la Tabla~\ref{distrospamela8}, una buena elección de variables puede generar una
descendencia muy fácil o muy difícil. En general, no es posible predecir cuál es
la mejor elección de variables a ser levantadas. Sin embargo la heurística
\vsids ha reportado muy buenos resultados en el mundo del \ssolving secuencial.


\begin{table}
\begin{footnotesize}
\begin{tabular}{ccrrrrr}
\toprule
Subconjunto & Cant. hijos & & & & & \\
de variables & no triviales & M\'aximo & Media & Suma total & Desv.
est. & Mediana \\
\cmidrule(r){1-7}
A 	& 136 	& 8.09 		& 1.13 		& 154.17	 	& 1.37 		& 0.51 \\
B 	& 136 	& 18.06 		& 1.28 		& 173.48	 	& 1.95 		& 0.72 \\
C 	& 192 	& 36.25 		& 8.74 		& 1678.88 	& 7.45 		& 5.78 \\
D	& 174 	& 63.62 		& 1.25 		& 218.29	 	& 6.27 		& 0.05 \\
E	& 192 	& 89.41 		& 2.18 		& 418.04	 	& 7.66 		& 0.39 \\
F 	& 192 	& 288.79	 	& 10.18 		& 1954.07 	& 22.36 		& 2.46 \\
G 	& 256 	& 376.70	 	& 86.03 		& 22024.53 	& 81.98 		& 56.98 \\
\bottomrule
\end{tabular}
\end{footnotesize}
\caption{Ejemplo: siete elecciones de variables distintas para partir un mismo problema. Tiempos secuenciales, en segundos, para los subproblemas resultantes de cada partición.}\label{distrospamela8}
\end{table}


Además de la determinación de qué variables levantar, es importante también
tener en cuenta cuántas levantar en cada momento. En primer lugar, es
importante tener en cuenta que la cantidad potencial de subproblemas generados
es exponencial con respecto a las variables levantadas. Esto afecta en un
doble sentido. En primera instancia es necesario observar que, dado que el
proceso de particionado se repite recursivamente, la generación de una
excesiva cantidad de subproblemas ante cada particionado genera una explosión
exponencial de problemas. Si este factor no es tenido en cuenta es altamente
probable que el sistema diverja y no sea capaz de arribar a una solución.
Sumado a esto, el tiempo insumido en generar los subproblemas producto de un
problema puede volverse excesivamente grande atentando una vez más contra el
paralelismo y la eficiencia de la herramienta.

Una vez más debemos recordar que la idea de levantar variables se basa en que,
ante una elección razonable de cuáles variables levantar, a mayor cantidad de
variables levantadas, más fáciles serán los subproblemas generados. Es por
esto que si la cantidad de variables a levantar es demasiado pequeña, es
probable que los subproblemas que sean generados no sean suficientemente 
fáciles como para ser resueltos y que, por lo tanto, sea necesario partirlos también en el
futuro haciendo que el tiempo invertido en intentar resolverlo se convierta en
tiempo desperdiciado. Además, si la cantidad de variables levantadas es muy
baja, existe la posibilidad de que la cantidad de subproblemas generados no
sea suficiente para aprovechar al máximo el \hard disponible.

La última observación referida al criterio de particionado tiene que ver con
qué problemas son dignos de ser distribuidos realmente. Hemos mencionado
muchas veces que al levantar $n$ variables, la cantidad \textbf{potencial} de
subproblemas generados es $2^n$. El uso de la palabra potencial tiene que ver
con el hecho de que varios de los subproblemas generados al levantar $n$
variables podrían ser resueltos trivialmente ¿A qué nos referimos con
trivialmente? En primer lugar a aquellos subproblemas en los que alcanza con
propagar\footnote{El uso del término propagación en este contexto es estricto.
Es decir que nos referimos exclusivamente a la realización de la clausura de
\bcp sobre el subproblema. Esta propagación no incluye la toma de nuevas
decisiones.} las implicaciones de las decisiones tomadas para obtener un
resultado --\sat o \unsat--. Pero más en general, nos referimos a todos
aquellos subproblemas en los que el \emph{overhead} de generar la tarea,
trasladarla a otro \w, cargar la tarea en el nuevo \w, y \solvear dicho
problema hasta arribar a un resultado sea suficientemente grande como para que
sea conveniente resolverlo directamente sin generar
una nueva tarea.

Se introduce entonces una nueva variable en el criterio de particionado que es
cuáles de los potenciales $2^n$ subproblemas generar como tareas a ser
resueltas por otros \ws y cuáles resolver directamente sin generarlos como nuevas
tareas. Se debe observar que \emph{no generar} un subproblema requiere invertir
un tiempo de cómputo suficiente para determinar el resultado de ese
subproblema. Una estrategia automática debe entonces decidir qué hacer con
esta variable teniendo en cuenta que invertir poco o nada de tiempo de cómputo
en filtrar los subproblemas puede degradar significativamente la eficiencia
del sistema debido a que una cantidad de subproblemas
demasiado fáciles serán generados como nuevas tareas. Pero también debe tener en cuenta que invertir demasiado
tiempo de cómputo en el filtrado de los subproblemas tiende a degradar
significativamente el paralelismo.

\subsubsection{Otros aspectos de interés}

Otro aspecto que influye en la \emph{performance} de la herramienta es la
forma en que se recorre el espacio de búsqueda. Cuando un \w queda ocioso debe
tomar una nueva tarea (si hubiera tareas pendientes). Surge entonces el
problema de determinar cuál de todas las tareas pendientes debe ser resuelta.
Como en los casos anteriores existen dos factores que deben ser balanceados.
Por un lado, si la estrategia (i.e. toma automática de decisiones) es adaptable según ciertas métricas sensadas dinámicamente, el
orden en el que se recorren las tareas a resolver alterará el resultado. Luego, nos gustaría poder
forzar a la herramienta a seguir el orden \emph{más adecuado} en relación a dichas métricas. En este contexto, la tarea \emph{ideal} puede encontrarse
almacenada en otro \w. Surge así el segundo factor a tener en cuenta. En aras
de la escalabilidad, el criterio para elegir la próxima tarea a ser resuelta no
debe generar una sobreutilización de la red de comunicaciones.

Un último factor a tener en cuenta es la necesidad de maximizar la utilización
del \hard disponible. Es decir que, idealmente, si hay \hard ocioso no debería
haber tareas pendientes, aún cuando estas se encuentren almacenadas en otros \w.

\subsection{La estrategia implementada}

Teniendo en cuenta los factores mencionados anteriormente nos volcamos al
desarrollo de una estrategia automática. Nuestro objetivo no estuvo puesto en
el desarrollo de una estrategia óptima ya que cada uno de los factores
mencionados con anterioridad podría ser objeto de un extenso y minucioso estudio. 
Más aún, no existen garantías de que tal estrategia óptima exista. Por lo tanto nuestro esfuerzo estuvo dedicado a no
incurrir en los errores más flagrantes, de modo que la herramienta funcionase
razonablemente bien con una serie de problemas y poder empezar a obtener
resultados que nos permitan resaltar los aspectos fuertes y los aspectos que
aún necesitan un esfuerzo importante.

En cuanto al criterio de particionado las decisiones tomadas fueron:

\begin{itemize}
	\item La cantidad de variables a levantar se fijó en 10. Aunque este valor puede ser alterado por el usuario.
	\item Las variables a levantar se seleccionan utilizando la heurística \vsids. Es decir que cuando un problema va a ser partido, se consulta la actividad de cada variable y se seleccionan --de acuerdo con el punto anterior-- las diez más activas.
	\item Cada potencial subproblema es \solveado por 50ms. Si en ese tiempo no se arriba a un resultado, se genera una nueva tarea pendiente.
\end{itemize}

Respecto al orden en el cual se recorre el espacio de búsqueda:

\begin{itemize}
	\item Si un \w queda ocioso y no tiene tareas pendientes en su espacio de almacenamiento, obtiene la tarea que corresponda de acuerdo al recorrido BFS --\emph{breadth-first search}-- del árbol de que representa el espacio de valuaciones aún no explorado.
	\item Si al quedar ocioso el \w cuenta con tareas pendientes en su espacio de almacenamiento, se selecciona la que corresponda por BFS de las tareas locales.
\end{itemize}

Sobre cuándo declarar que un problema es demasiado difícil, adoptamos una
estrategia basada en el estado del sistema. En concreto nos enfocamos en que
el sistema mantenga una cierta tasa de progreso. Para ello medimos la cantidad
de subproblemas resueltos por unidad de tiempo y luego establecimos un umbral
objetivo. De esta forma, si la frecuencia actual se encuentra por debajo del umbral
objetivo, entonces debemos escojer uno de los problemas que actualmente están
siendo resueltos y partirlo.

Llamaremos \textbf{frecuencia objetivo} al umbral, entendido como 
cantidad de problemas resueltos por \w, por segundo por debajo del cual se decide que algún
problema debe ser partido. Llamaremos \textbf{frecuencia actual} a un valor
que estima la cantidad de problemas terminados por \w, por segundo a partir de
promediar los problemas que han terminado durante los últimos $t$ segundos.
Llamaremos \textbf{tamaño de la ventana de muestreo} al mencionado valor $t$.
Llamaremos \textbf{frecuencia de observación} a la frecuencia con la que se revisa
si la frecuencia actual se encuentra por encima de la frecuencia objetivo.

En concreto los parámetros utilizados fueron:

\begin{itemize}
	\item Frecuencia objetivo: $0.15$ p/\w p/seg.
	\item Tamaño de la ventana de muestreo: $20s$
	\item Frecuencia de observación: cada $5s$
\end{itemize}

Que la frecuencia objetivo dependa de la cantidad de \ws permite ajustar la
cantidad de problemas generados de acuerdo a la capacidad de consumirlos que
tiene el sistema. Es importante notar que el tamaño de la ventana influye en
cuánto peso se le da a la historia. Tomar una ventana de mayor tamaño exhibirá un comportamiento más estable en el tiempo,
mientras que una ventana más pequeña se adapta más rápidamente y consecuentemente mostrará mayor variabilidad. La frecuencia de observación administra cuánto tiempo de
tolerancia se tiene ante una frecuencia actual menor a una frecuencia
objetivo. Si la frecuencia de observación es alta --menor valor-- entonces la
tolerancia será baja y ante cualquier caída de la frecuencia actual por debajo
del objetivo el sistema reaccionará. Por otro lado, una frecuencia baja --mayor
valor-- representará una tolerancia mayor, dando oportunidad al sistema de
tener períodos en los que la frecuencia actual no alcance la frecuencia
objetivo. La combinación de estos tres parámetros configura la agresividad
del sistema a la hora de determinar que un problema es demasiado difícil.

Además del criterio por frecuencia decidimos que, ante la situación en la que hay
\ws sin tareas para realizar y en el sistema no hay más tareas pendientes, se
debe partir un problema en curso. De este modo evitamos que haya recursos
ociosos. En particular esta decisión es útil al comienzo de la ejecución de un
problema para distribuir rápidamente el trabajo entre las distintas unidades
de cómputo.

Falta determinar entonces cuál de los problemas en curso será el seleccionado
para ser partido. En este punto optamos por aquel en el que se ha
invertido más tiempo de cómputo. De este modo evitamos que un \w quede atorado
intentando resolver un problema que podría ser extremadamente difícil, a la vez
que procuramos evitar que un problema sea partido demasiado pronto.

Con respecto al balanceo del almacenamiento se tomó la decisión de que cada
vez que un \w queda ocioso y sin tareas locales pendientes, además de obtener la
tarea que corresponde por BFS, el \w obtendrá una conjunto de tareas pendientes
de aquel \w que posea mayor cantidad. Llamemos $w_{idle}$ al \w que quedó sin
tareas y $w_{overwhelmed}$ al que más tareas pendientes posee, entonces la
cantidad de tareas transferidas en esta ocasión será:

\begin{equation}
\frac{\#tareas\_pendientes(w_{overwhelmed})}{\# workers}
\label{eq:share}
\end{equation} 

La idea detrás de esto es que $w_{idle}$
obtiene \emph{la porción que le corresponde} de las tareas de
$w_{overwhelmed}$. Para evitar transferencias extremadamente largas, la
cantidad expresada en Eq.-\ref{eq:share} satura en $10$ tareas. Para evitar
una mala distribución, las tareas que conforman una porción a ser transferida
son seleccionadas al azar del conjunto de tareas pendientes de
$w_{overwhelmed}$. Vale la pena notar que si un \w solicitara las $n$ tareas que mejor califican según el criterio de elección, éste concentraría un conjunto de tareas de alta prioridad, generando un potencial problema de contención.

\section{Resultados experimentales}

En esta sección presentamos los experimentos realizados con el fin de evaluar
el desempeño de nuestra herramienta.  Los experimentos se ejecutaron en el
\cluster CeCAR \cite{cecar}. Se utilizaron 17 equipos,
cada uno de los cuales cuenta con dos procesadores \texttt{Intel Dual Core Xeon
5030 2.67 Ghz}, \texttt{2 GB} de memoria RAM y un disco rígido \texttt{Western
Digital WD800JD 80Gb SATA 7200RPM}. Uno de los equipos fue utilizado para
ejecutar el tablero de control y el \master, mientras que se ejecutaron 4 \ws
en cada uno de los restantes 16 equipos totalizando 64 \ws.


\subsection{Casos de estudio}

Debido a la orientación del grupo de investigación, nos interesa evaluar la herramienta principalmente en lo que respecta a su uso para la verificación formal de software. En la práctica, nos interesa hacerlo sobre todo para problemas resultantes de la traducción de especificaciones de sistemas y propiedades en el lenguaje Alloy; esto es, de modo de evaluar la viabilidad de usar la herramienta para capitalizar la disponibilidad de \emph{hardware} ocioso y reemplazar el uso de la herramienta secuencial Alloy Analyzer en aquellos casos en que esta última demora demasiado en demostrar una propiedad (o en que sus capacidades directamente se ven excedidas, de modo que el problema no resulta tratable en secuencial).

Otro aspecto de los problemas difíciles provenientes de especificaciones Alloy que resulta interesante para conformar un \emph{benchmark} es que pueden ser generados en distintos grados de dificultad a medida que se elevan las cotas para los dominios de datos atómicos sobre los que predican. Como se mencionó en el Cap.~\ref{intro}, para que el análisis resulte decidible se procede a acotar dichos dominios (así, las propiedades se demuestran automáticamente pero, por ejemplo, ``\ldots para hasta 12 trenes, 8 semáforos y 8 pasos a nivel''). En el caso de Alloy, estas cotas suelen denominarse \emph{scopes}. En otras palabras, el problema de verificar cierta propiedad sobre cierto modelo da origen a una sucesión o familia de problemas (para \emph{scopes} cada vez mayores) que son estructuralmente similares pero ostentan cantidades de variables, cláusulas y tiempos de \emph{solving} secuencial estrictamente crecientes.

Otra potencial fuente de problemas que resulta natural considerar son los \emph{benchmarks} de las competencias anuales de \ssolving. Sin embargo, la forma en que tradicionalmente se evalúan las herramientas en dichos eventos consiste en medir, para cada \ssolver $S$ y para cada cantidad de tiempo $t$, la \emph{cantidad} de problemas que $S$ logra resolver en $t$ segundos. (Esto se debe a que algunos problemas fáciles para un \emph{solver} son difíciles para otros y viceversa.) En consecuencia, los \emph{benchmarks} de las competencias suelen incluir gran cantidad de problemas muy pequeños, pequeños y medianos cuyo tratamiento optimizado escapa al alcance de nuestra herramienta.


\subsubsection{\emph{Garbage collection} usando Mark \& Sweep}

Entre los casos de estudio provistos con el programa Alloy Analyzer se incluye un modelo formal de una implementación de \emph{garbage collection} basada en el algoritmo conocido como ``mark and sweep''. El modelo incluye una representación del \emph{heap} del sistema, dominios de datos \texttt{HeapState} y \texttt{Node}, una especificación del algoritmo y propiedades \texttt{Soundness} y \texttt{Completeness}.

La propiedad de \texttt{Soundness} afirma que, tras haber ejecutado el algoritmo de \emph{garbage collection}, no hay intersección entre el conjunto de nodos alcanzables en el \emph{heap} y la lista de nodos libres. La propiedad de \texttt{Completeness} afirma que, tras haber ejecutado el algoritmo, todo nodo inalcanzable en el \emph{heap} efectivamente aparece en la lista de nodos libres.

Hemos incluido ambos problemas en el \emph{benchmark} para la evaluación de la herramienta. Ambos son problemas difíciles que requieren exponencialmente más tiempo de análisis conforme crece la cantidad de \texttt{Node}s en el \emph{scope}. Sin embargo, el segundo problema (el de demostrar \texttt{Completeness}) exhibe una curva de dificultad aún más pronunciada que el primero.


\subsubsection{Ruteo en redes heterogéneas}

Este caso proviene de un modelo formal de ruteo en redes heterogéneas, desarrollado en AT\&T, que involucra agentes móviles, identificadores, rutas y múltiples dominios de red. El modelo en cuestión incluye diversas propiedades, de las cuales la última, \texttt{StructureSufficientForPairReturnability}, resulta particularmente costosa de demostrar. Para más detalles referimos al lector al artículo ``Compositional Binding in Network Domains'' \cite{zave:fm06}.


\subsubsection{Clausura relacional reflexo-transitiva}

Se trata de un modelo sencillo que involucra un dominio de datos $A$ y una relación $R \subseteq A \times A$, a la que (sólo) se le exige que sea reflexiva y transitiva. La propiedad a verificar afirma que $R^{*} = R$, esto es, que la clausura reflexo-transitiva de una relación reflexiva y transitiva es exactamente esa relación. Esta es una propiedad válida, cosa que podría probarse matemáticamente, por fuera del modelo. El interés radica en que se trata de un modelo breve y sencillo cuya traducción engendra problemas con cantidades muy pequeñas de variables proposicionales, pero que a pesar de ello resultan de considerable dificultad para los \ssolvers y exhiben una curva exponencial (tiempo de \solving para $|A|$ creciente) relativamente pronunciada.


\subsection{Resultados obtenidos}


La Tab.~\ref{tab:resultados} muestra la comparación entre el tiempo de
ejecución requerido por el \ssolver secuencial (columna ``seq.~walltime~(s)'')
y el tiempo percibido requerido por nuestra herramienta (columna
``avg.~par.~walltime~(s)''). Los tiempos reportados para el caso distribuido
se calcularon tomando el promedio de varias corridas. La columna ``reps''
indica la cantidad de repeticiones realizadas para cada problema. La columna
``speedup'' es el resultado de la siguiente división:
$$\frac{\text{seq.~walltime~(s)}}{\text{avg.~par.~walltime~(s)}}$$ Este valor
denota la mejora relativa obtenida mediante la utilización de nuestra
herramienta. La columna ``efficiency'' es el resultado de la siguiente división:
$$\frac{\text{speedup}}{\text{\#\ws}}$$
Esta medida indica la ganancia neta de \emph{speedup} por \w utilizado.

\begin{table}
	\footnotesize
	\begin{tabular}{lrrrrrr}
		\toprule
		problem	&	scope	&	seq. walltime (s)	&	reps. & avg. par.	&	speedup	&	efficiency \\
			&		&	&	 & walltime (s)	&		&	 \\
		\cmidrule(r){1-7}
		Pamela	&	8	&	308.26	&		7 & 60.46	& 5.10x	&	0.08 \\
		Pamela	&	9	&	76168.16	&	6& 407.34	& 	186.99x	&	2.92 \\
		Pamela	&	$^*$10	&	$>$1209600.00	&	5 & 5046.84	&	$>$239.67x	&	$>$3.74 \\
		\cmidrule(r){1-7}
		Closure	&	11	&	749.65	&	14 & 291.28	&	2.57x	&	0.04 \\
		Closure	&	12	&	3983.36	&	5 & 1914.45	&	2.08x	&	0.03 \\
		Closure	&	13	&	16261.35	& 5 &	4362.61	&	3.73x	&	0.06 \\
		\cmidrule(r){1-7}
		MarkGC Soundness2	&	9	&	217.31	&	10 & 200.85	&	1.08x	&	0.02 \\
		MarkGC Soundness2	&	10	&	 2855.30	&	7 &1376.89	&	2.07x	&	0.03 \\
		\cmidrule(r){1-7}
		MarkGC Completeness	&	8	&	180.25	&	7 & 61.50	&	2.93x	&	0.05 \\
		MarkGC Completeness	&	9	&	18643.06	&	5 & 1825.60	&	10.21x	&	0.16 \\
		\bottomrule
		\\
		\multicolumn{7}{l}{\begin{tiny}$^*$: Se ejecutó por 14 días sin que el \ssolver secuencial consiga un resultado\end{tiny}}
	\end{tabular}
	\caption{Tiempo de ejecución (en segundos) distribuido vs. secuencial}
	\label{tab:resultados}
\end{table}

En primera instancia corresponde aclarar por qué la cantidad de repeticiones
realizadas para cada problema son diferentes. Esto se debe a que procuramos
realizar la mayor cantidad de repeticiones posibles para cada problema. Por lo
tanto, la cantidad de repeticiones para los problemas más grandes fue menor
que la de los problemas más pequeños.

Una de las hipótesis de trabajo es la disponibilidad de \hard y, en particular, la de clusters de cómputo intensivo que pueden ser usado para validar y/o verificar software. 

Respecto a los resultados obtenidos, el \emph{speedup} que se exhibe en la tabla, pero sobre todo en relación a los datos consignados en la columna \emph{efficiency}, no sólo no muestra una mejora importante si se considera que es la resultante de utilizar 17 unidades de cómputo; pero debe destacarse que:
\begin{itemize}
\item en general el \emph{speedup} es creciente respecto del tamaño de los problemas que conforman el \emph{benchmark} indicando esto que el enfoque resulta apropiado para resolver problemas de gran tamaño que no son tratables en el contexto de \ssolving secuencial,
\item salvo los casos extremadamente positivos correspondientes al caso de estudio Pamela, y el caso MarkGC Soundness2 para scope 9, en que no se percibe mejoría, el rendimiento del enfoque de resolución reporta una ganancia mínima de $2.07x$, un promedio de $4.35x$ y una ganancia máxima de $10,21x$.
\end{itemize}

La Tabla~\ref{tab:incremental} muestra el impacto que tiene el agregado de
nuevas unidades de cómputo sobre el desempeño del nodo en el que fueron
ejecutados tanto el \master como el Tablero de control. Cabe mencionar que
este nodo, y las tareas que realiza, son el único potencial cuello de botella
que presenta la herramienta. De los datos consignados se puede observar no
sólo una utilización objetivamente muy baja de los recursos disponibles en
dicho nodo, sino que el agregado de nuevos \w de cómputo no exhibe un
crecimiento que ponga en riesgo la escalabilidad de la herramienta.

\begin{table}
	\centering
	\begin{tabular}{lrrrrrr}
		
			\ws 				&	8 		&	16		&	32 		&	64	\\
		\toprule	
			max. memory(1) (KB)	&	11344	&	11328	&	12152	&	10844 \\
			max. memory(2) (KB)	&	12784	&	12696	&	12728 	&	13104 \\
			system load	(\%)	&	$<1$	&	$<1$	&	$<1$ 	&	$<1$ \\
		\bottomrule \\
		\multicolumn{5}{l}{\begin{tiny}(1): Tablero de control. (2) \master \end{tiny}}
	\end{tabular}
	\caption{Utilización de recursos del Tablero de control y del \master para cantidad de \ws crecientes}
	\label{tab:incremental}
\end{table}

En términos generales la herramienta consiguió disminuir el tiempo necesario
para resolver un problema pero en casi todos los casos lejos del ideal
esperado en proporción a la cantidad de nodos agregados. Respecto de esto
nuestra conjetura es que la partición de un problema en subproblemas
independiza dos partes de su problema ``padre'' y las considera como nuevas
tareas. Si aceptamos la hipótesis de que las cláusulas aprendidas guían al
\ssolver en el recorrido del espacio de valuaciones para intentar evitar
aquellas que no producirán un resultado, todas aquellas cláusulas que el
\ssolver aprendió a lo largo del proceso de \solving del padre debieran jugar
un rol en la posibilidad resolver los subproblemas que de él derivan.

Así, en el capítulo siguiente afrontaremos la tarea de evaluar la viabilidad
de implementar técnicas de aprendizaje de cláusulas guiado por conflictos como
parte de la herramienta que hemos desarrollado
