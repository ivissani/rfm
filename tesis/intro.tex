%!TEX root = tesis.tex

\chapter{Introducción} 
\label{intro}

En la actualidad el \soft forma parte integral de nuestra vida. Este interviene en todo tipo de tareas. Desde el envío de un mensaje de texto
o la realización de una llamada telefónica hasta el sistema de control de una
central nuclear, pasando por el control de un ascensor o el sistema de
velocidad crucero de un automóvil, utilizan sistemas basados en \hard y \soft
informático. La variedad, alcance y \textbf{criticidad} de las
responsabilidades asignadas a las piezas de \soft hacen de las mismas un
componente de alto impacto en nuestra realidad cotidiana.

Por otra parte, la alta complejidad del \soft hace que la construcción del
mismo sea una tarea propensa a errores. El impacto de una falla en un
componente de \soft puede variar desde la imposibilidad de utilizar un
artefacto doméstico hasta una catástrofe de magnitudes como la fundición del
núcleo de un reactor nuclear.

Son estos aspectos los que motivan la necesidad creciente de construir métodos
(formalismos, metodologías, herramientas, etc.) que permitan garantizar la
calidad del \soft en un sentido general. La posibilidad de establecer la
ausencia de errores en un programa radica en la capacidad de establecer
inequívocamente que ese programa cumple determinadas propiedades que expresan
el adecuamiento del mismo al comportamiento esperado. Por ejemplo si
quisiéramos establecer el correcto  funcionamiento del mecanismo de seguridad
de un horno de microondas podríamos enunciar una propiedad como la siguiente:
``No es posible encender el microondas si la puerta está abierta''.

La \textbf{verificación formal} de \soft consiste en demostrar que un
determinado sistema o algoritmo es correcto respecto a una
\textbf{especificación formal} del mismo. Los lenguajes naturales son
intrínsecamente ambiguos y por lo tanto una descripción de un sistema
realizada en lenguaje natural resulta muy difícil de verificar. En lo
subsiguiente asumiremos que una especificación será formal en tanto está
descripta en un lenguaje que posee:  \begin{inparaenum}[a)]  \item reglas para
determinar cuándo una sentencia pertenece al lenguaje (sintaxis),  \item
reglas que permitan interpretar las sentencias (bien formadas) de manera
precisa y significativa (semántica), y  \item reglas para inferir información
útil a partir de la especificación (\emph{proof theory}).  \end{inparaenum}

El enfoque de la verificación formal de software, tal vez uno de los más
ambiciosos en el campo de la ingeniería de software, enfrenta dos problemas
fundamentales. En primer lugar está la dificultad de construir una
especificación formal que exprese de manera correcta el comportamiento
esperado del sistema (a lo que llamaremos validación) ya que el mismo
normalmente proviene de requerimientos expresados en lenguaje natural. El
segundo problema fundamental es que las lógicas subyacentes a la mayoría de
los lenguajes formales capaces de expresar un conjunto lo suficientemente
amplio e interesante de propiedades son normalmente indecidibles, es decir que
no existe un algoritmo capaz de, para toda fórmula del lenguaje, establecer si
la misma es un teorema o no.

El problema de la indecidibilidad se transforma en un escollo fundamental en
tanto impide la construcción de métodos de verificación formal completamente
automáticos. Diversos enfoques han sido desarrollados para atacar este
problema \todo[inline]{Buscar citas. SMT Solving, Bounded MC, etc}. La mayoría
de estos métodos consisten en restringir de alguna manera el lenguaje formal
utilizado de modo de generar un sublenguaje del mismo que sea decidible. Uno
de los enfoques desarrollados en este sentido es el conocido como \bmc
(verficación acotada de modelos).

\mc\cite{emerson:scp-2_3} es una técnica desarrollada para la verificación de
sistemas reactivos. La técnica consiste en la exploración exhaustiva y
automática del espacio de estados de un sistema modelado como una máquina de
estados finita. Cuando la cantidad de estados del sistema es relativamente
grande la exploración exhaustiva del espacio de estados se vuelve
inpracticable. Esto motivó el desarrollo de la técnica conocida como
\smc\cite{burch:lics90, mcmillan93} que ataca el problema antedicho a partir
de evitar construir el grafo de estados del sistema utilizando fórmulas
lógicas para caracterizar conjuntos y relaciones. Cuando se trata de fórmulas
proposicionales, en \smc el chequeo de las fórmulas es realizada típicamente
mediante el uso de BDDs (\bdds)\cite{bryant:ieeetc-8}, una forma canónica de representar dichas
fórmulas. A pesar de que las técnicas de \smc basadas en  BDDs son capaces de
manejar sistemas con cientos de estados, el tamaño de los BDDs generados para
sistemas grandes se vuelve prohibitivo.

% \marginpar{BMC no
% está vinculado unívocamente a sat-based. BMC aparece, como dijiste antes, para
% lidiar con los lenguajes indesidibles mientras que sat-based es una manera de
% hacer tanto MC como BMC aprovechando los avances en ese área en particular.
% Esto último es el argumento de quienes usan SS off-the-shelf. Yo acá diría
% testo último.}

El problema del espacio requerido por los BDDs motivó la creación de la
técnica conocida como \bmc\cite{Biere:1999:SMC:646483.691738}. La técnica
presentada en \cite{Biere:1999:SMC:646483.691738} consiste en buscar
contraejemplos de una longitud acotada $k$ a partir de generar una fórmula
proposicional que sea verdadera si y sólo si un contraejemplo de dicha
longitud existe. Para verificar estas fórmulas los autores reemplazaron la
utilización de BDDs por la utilización de procedimientos SAT ya que para esa
época los mismos eran capaces de manejar fórmulas proposicionales con varios
miles de variables y no sufrían del problema de la explosión en memoria. Si
bien la técnica mencionada es correcta y completa para sistemas modelados como
máquinas de estados finitos ya que el espacio de búsqueda, aunque usualmente
sea exponencialmente grande, es finito, no es cierto que esta metodología
aplicada a un lenguaje formal cualquiera sea siquiera correcta: tómese, por
ejemplo, cualquier lenguaje lógico que no posea la propiedad de modelo finito.
En tales casos existen conjuntos de fórmulas cuyos modelos son necesariamente
infinitos; así, la exploración de un subespacio finito en busca de un modelo
nunca podría garantizar una respuesta negativa. Esta debilidad, en el caso general,
generada por la introducción de la cota en el tamaño del contraejemplo es lo
que, aplicado a un sistema lógico indecidible, permite obtener un subconjunto
del lenguaje original que resulte decidible y, por lo tanto, automáticamente
verificable, aun cuando las respuestas del procedimiento utilizado resulten
parciales respecto de la validez de una propiedad determinada en el lenguaje
original.

Un ejemplo típico de esto se da en lenguajes de primer orden donde la
indecidibilidad se da a partir de la existencia de cuantificadores
existenciales sobre dominios no acotados. En estos casos la cota en el tamaño
del contraejemplo puede ser la resultante de establecer una cota a cada uno de
los dominios que aparecen en la signatura. De este modo las cuantificaciones
existenciales se convierten en disyunciones y por lo tanto el lenguaje se
vuelve decidible (y en particular codificable como una fórmula proposicional
susceptible de ser verificada automáticamente). Por ejemplo la fórmula:
\begin{equation}(\exists x)\ (\alpha(x))\end{equation}  \noindent a partir de
la introducción de la cota sobre el dominio del cual toma valores $x$ se
transforma en la fórmula finita:  \begin{equation}\left(\exists t \in
\{t_1,\ldots,t_n\}\right)\ \left(\alpha(t)\right)\end{equation}  \noindent o
lo que es lo mismo:  \begin{equation} \alpha(t_1) \vee \ldots \vee
\alpha(t_n)\end{equation}

Cabe mencionar que esta restricción al tamaño de los contraejemplos
introducida por forzar que la interpretación de los dominios sea sobre
conjuntos finitos hace que el método de análisis no sea correcto ya que la
ausencia de contraejemplos a una propiedad determinada dentro de ciertas cotas
no garantiza la ausencia de contraejemplos para cotas de mayor tamaño. Por
esta razón, \bmc no puede ser considerada una técnica de verificación en el
sentido estricto. Sin embargo la utilización de \bmc permite ganar confianza
en la propiedad que se quiere verificar antes de embarcarse en la costosa
tarea de demostrarla formalmente. Algunas aplicaciones exitosas de \bmc son la
generación automática de casos de test, validación de código y
especificaciones, etc.\todo{citas}

Existen una variedad de herramientas formales que utilizan el enfoque de \bmc
basado en sat-solving que mencionamos en los párrafos anteriores. Una metodología
ampliamente difundida y adoptada es Alloy \cite{jackson:acmtosem-11_2}. Alloy es un lenguaje
formal de primer orden diseñado para expresar las propiedades estructurales de
un sistema; posee una sintaxis declarativa cercana a los lenguajes orientados
a objetos y lo suficientemente poderosa como para expresar propiedades
complejas y a la vez plausible de ser analizada de forma completamente
automática (utilizando la herramienta homónima) a partir de una traducción
como la que mencionamos en los párrafos precedentes.


\section{SAT Solving}

El interés en construir procedimientos sistemáticos y automáticos de
demostración se remonta al interés que tuvieron diversos matemáticos en
construir un método de esas características para la lógica de primer orden.
Uno de los máximos exponentes fue David Hilbert, quien enunció que la búsqueda
de un algoritmo de decisión para la lógica de primer orden era el problema
central de la lógica matemática.\marginpar{Esta aseveración es medio fuerte
porque métodos matemáticos en el sentido de algoritmo para resolver cosas hay
desde hace mucho y decir que la cosa empezó con Hilbert en el 1900 es un cacho
fuerte.} La demostración de que un procedimiento de estas características no
era posible elaborada por Church y Turing provocó una pérdida de interés en
esta clase de problemas. Sin embargo la construcción de procedimientos, no ya
de decisión, sino de demostración para la lógica de primer orden (entre los
cuales se destaca el desarrollado por Martin Davis y Hilary Putnam
\cite{Davis:1960:CPQ:321033.321034}) sumada a los crecientes campos de
aplicación, dentro de las ciencias de la computación, de lógicas decidibles o
de fragmentos decidibles de lógicas no decidibles, dieron un nuevo impulso al
desarrollo de estos métodos.

En particular la variedad de aplicaciones que fueron surgiendo para la lógica
proposicional (\emph{Planning} en inteligencia artificial, demostración
automática de teoremas, \mc, verificación de \soft, etc.)
impulsaron fuertemente el desarrollo de técnicas y procedimientos para
determinar si una fórmula de la lógica proposicional podía ser verdadera. El
establecimiento de este problema como el primer problema \npc
\cite{Cook:1971:CTP:800157.805047} no impidió el desarrollo de heurísticas que
hoy permiten manejar fórmulas de cientos de miles de variables eficientemente.

Si bien existe una variedad de métodos para determinar si una fórmula
proposicional puede ser verdadera, los más utilizados son aquellos derivados
del algoritmo desarrollado por Davis, Longemann y Loveland (\dpll) en
\cite{Davis:1962:MPT:368273.368557} que es un refinamiento del algoritmo
presentado en \cite{Davis:1960:CPQ:321033.321034}. Este procedimiento es, en
esencia, un algoritmo de \bt.

Tuvieron que pasar varios años para que Marques-Silva introdujera en \cite
{marques-silva:iccad96} la primera optimización radical al algoritmo \dpll que
dio origen a lo que hoy se conoce como procedimiento \cdcl (\CDCL) cuya base
es el aprendizaje de nuevas cláusulas ante cada conflicto. Ésto permite
recortar el espacio de búsqueda de manera dramática. A partir de ese momento
una serie de mejoras han sido desarrolladas para cada una de las partes
críticas del algoritmo desarrollado por Marques-Silva\footnote{Para un
relevamiento completo de las optimizaciones aplicadas al algoritmo \CDCL ver
\cite{manthey:mathesis}}.

Una de las partes fundamentales de los algoritmos de \ssolving es la conocida
como \bcp (o simplemente propagación). Cuando una fórmula $\varphi$ de la
lógica proposicional es una conjunción, para que $\varphi$ sea
verdadera, cada una de sus partes deben serlo. Es por esto que si una
de sus partes es una fórmula que contiene únicamente un literal, ese literal
debe necesariamente ser verdadero para que $\varphi$ pueda serlo. Bajo
esta observación se desarrolló el procedimiento de \BCP que consiste
precisamente en \emph{propagar} una decisión (asignación de valor de verdad a
un literal) observando qué partes de la fórmula quedan con un único literal
libre a partir de esta decisión. Luego esos literales deben ser necesariamente
asignados al valor de verdad \T convirtiéndose en \emph{implicaciones} de la
decisión tomada.

A pesar de la enorme mejora que representó la incorporación de aprendizaje
propuesta por Marques-Silva, con el correr del tiempo se estableció que los
\ssolvers invierten la mayor parte de su tiempo (cerca del $90\%$) en propagar
las implicaciones que se siguen de cada una de las decisiones tomadas durante
la búsqueda. Esta propagación incluye el recorrido del conjunto de cláusulas
que conforman el problema cuya complejidad es, en mejor caso, lineal. De esta
forma, la incorporación de nuevas cláusulas (el \emph{leitmotiv} del
procedimiento \CDCL) a la vez que proporciona una ventaja al recortar el
espacio de búsqueda representa una desventaja en tanto que incrementa la
cantidad de cláusulas sobre las cuales es necesario realizar las
propagaciones.

Diversas optimizaciones se han desarrollado para reducir el tiempo invertido
en la propagación. Una de las más importantes es la presentada en
\cite{moskewicz:da01} que permite reducir drásticamente el tiempo de
propagación a partir de mantener una referencia a dos literales no asignados
de cada cláusula del problema. De esta forma, una cláusula se vuelve
(potencialmente) unitaria únicamente cuando uno de los dos literales
\emph{observados} es asignado. De otra manera la cláusula posee al menos dos
literales libres y por lo tanto no se produce una implicación a partir de
dicha cláusula.

Más allá de las optimizaciones presentadas, el tiempo de propagación sigue
dominando el tiempo de procesamiento utilizado por los \ssolvers secuenciales.
Esta situación, sumado al hecho de que la incorporación de nuevas cláusulas
también representa un problema desde el punto de vista de la utilización de la
memoria de la computadora, han llevado a la comunidad a establecer que, por lo
general, es mejor tener un política agresiva de purga de la base de datos de
cláusulas aprendidas \cite{Audemard:2009:PLC:1661445.1661509}. Si bien se han
desarrollado distintas heurísticas con muy buenos resultados, el problema de
determinar cuántas y cuáles cláusulas descartar (o su equivalente, preservar)
sigue siendo un problema ampliamente estudiado en la comunidad \sat.


\section{SAT Solving paralelo y distribuido}

El establecimiento del problema \sat como un problema \npc hace pensar que no es
posible obtener un algoritmo polinomial que resuelva dicho problema. Al mismo
tiempo, en los últimos años, las mejoras a los algoritmos secuenciales de
\ssolving han sido de carácter marginal y no parece haber avances que permitan
esperar un salto en el orden de magnitud de los problemas resueltos por los
\ssolvers secuenciales. Por otro lado, la rapidez en el incremento de la
velocidad de las computadoras ha ido disminuyendo (esencialmente debido al
problema de la disipación del calor y el consumo de energía) \marginpar{Agregar
alguna cita o gráfico que sirvan de argumento.} y en su lugar se ha producido un
incremento en la cantidad de unidades de procesamiento disponibles así como su
abaratamiento. Estos hechos han impulsado a la comunidad \sat a explorar la
posibilidad de construir algortimos paralelos y/o distribuidos de \ssolving de
modo de: \begin{inparaenum}[a)]  \item sacar el máximo provecho de la nueva
tendencia en construcción de computadoras y  \item proveer un algoritmo
\textbf{escalable} que permita aprovechar el abaratamiento del \hard
\end{inparaenum}.

La aparición de los primeros \ssolvers paralelos y distribuidos
\cite{bohm:1996:afast, zhang:jsc-1996} es contemporánea a la aparición del
algoritmo \CDCL. Por lo tanto los primeros \ssolvers paralelos fueron
desarrollados sobre la base del algoritmo \dpll y es por esta razón que estos
\ssolvers no incorporaban aprendizaje de nuevas cláusulas.

Una de las razones que dieron origen a implementaciones tempranas de
algoritmos paralelos para el problema \sat es que este problema presenta una
forma intuitiva y fácil de ser dividido. La idea consiste en seleccionar un
subconjunto pequeño de variables del problema, digamos $n$, y generar las
$2^n$ asignaciones posibles de valores de verdad a este subconjunto. Cada una
de estas asignaciones es una valuación parcial del problema original y por lo
tanto lo que obtenemos son $2^n$ subproblemas \textbf{disjuntos} que pueden
ser resueltos por distintas unidades de procesamiento. Cada unidad de
procesamiento puede utilizar entonces alguna implementación de un algoritmo de
\ssolving secuencial para resolver el subproblema que le haya tocado. Esta
técnica, llamada \gp\cite{zhang:jsc-1996}, introduce una pregunta fundamental:
¿De qué modo determinar un \gp efectivo? Esto es, cómo construir un \gp que
distribuya equitativamente la carga de trabajo entre las distintas unidades de
procesamiento.

\newcommand{\pfolio}{\emph{portfolio}\xspace}

El enfoque anterior de división de un problema \sat es adecuado para un
escenario con unidades de procesamiento que se comunican a través de una red,
como pude ser un \emph{cluster} o en computación \emph{grid}. La proliferación
de computadoras con múltiples procesadores y múltiples núcleos impulsó la
investigación sobre formas de paralelización de \ssolvers en un escenario
donde las múltiples unidades de procesamiento tienen acceso uniforme a una
memoria común. Este nuevo escenario permitió el desarrollo de una forma
radicalmente distinta de paralelización de un problema \sat. La misma consiste
escencialmente en lo que se conoce como \pfolio. La idea se fundamenta en la
observación de que distintos \ssolvers secuenciales, o un mismo \ssolver con
distintos parámetros, pueden resolver un mismo problema en tiempos sumamente
diferentes. Sobre esta base se desarrollaron \ssolvers paralelos que en lugar
de dividir el problema a resolver, ejecutan distintos \ssolvers sobre el mismo
problema (o un mismo \ssolver con distintos parámetros\cite{hamadi09}) y
comparten las cláusulas aprendidas entre las distintas instancias. Es
importante notar que así como en los \ssolvers secuenciales, la incorporación
de aprendizaje es un factor fundamental en la mejora del rendimiento, en el
contexto de \ssolving paralelo con el enfoque \pfolio también lo es. Esto se
debe a que sin aprendizaje compartido entre las distintas instancias el
resultado del enfoque \pfolio sería, en el mejor caso, tan bueno como el mejor
de los \ssolvers utilizados. Por el contrario, la incorporación de aprendizaje
compartido permite potencialmente obtener mejores resultados que el del mejor
\ssolver utilizado. La información experimental ha confirmado esta hipótesis.

\newcommand{\mcore}{multi\emph{core}\xspace}
\newcommand{\solvers}{\emph{solvers}\xspace}

Por supuesto, distintas implementaciones concretas han sido desarrolladas
sobre esta idea, e incluso implementaciones híbridas que combinan las dos
formas de paralelización antedichas\todo{Faltan citas}. Sin embargo, a lo
largo de los años, ha tomado un impulso mucho mayor el enfoque basado en
memoria compartida. Esto puede verse por un lado en la disponibilidad de
herramientas. Hoy en día existen por lo menos una decena de \ssolvers
paralelos para procesadores \mcore que son activamente desarrollados y que
compiten año a año en las competencias internacionales de \ssolving. Por otro
lado el sólo hecho de que las competencias internacionales de \ssolving hayan
incorporado categorías específicas para \solvers paralelos (por lo menos desde
el año 2008) es también un síntoma de la alta actividad que se viene
desarrollando en este área. Por el contrario, el interés en los \ssolvers
distribuidos no ha obtenido la misma atención por parte de la comunidad. De
hecho, la mayoría de los \ssolvers distribuidos han sido abandonados. Es
sumamente difícil encontrar herramientes de este campo disponibles para ser
utilizadas, y las que existen han quedado obsoletas.

Es importante destacar que el enfoque \pfolio ha mostrado resultados
prometedores en la resolución más eficiente de problemas basados en \sat y los
datos de estos avances no sólo están documentados en numerosas publicaciones
que lo avalan, sino también en el surgimiento y crecimiento de la comunidad
que compromete sus esfuerzos en esta nueva área de investigación.
\marginpar{citas} Es nuestra opinión que este enfoque no da respuesta al hecho
de que hoy la tendencia indica que la escalabilidad proviene del trabajo
cooperativo y distribuido de unidades independientes de cómputo.

El objetivo de esta tesis es la de dar respuesta a la construcción de un \ssolver paralelo y distribuido basado en la técnica \gp que: '
\begin{itemize}
\item realice una adecuación y balanceo de la carga de procesamiento en \emph
{run-time} sobre una cantidad y configuración de equipamiento desconocida,
\item realice una adecuación y balanceo del uso de memoria primaria y
secundaria en \emph{run-time}, también sobre una cantidad y configuración de
equipamiento desconocida, y 
\item sea extensible y fácil de parametrizar en
todos aquellos aspectos que determinen el comportamiento del procedimiento de
resolución del problema. 
\end{itemize}

Al mismo tiempo, someteremos a prueba la implementación de un método de
aprendizaje distribuido utilizando distintos criterios de aprendizaje con el
objetivo de identificar si esta técnica que ha demostrado ser revolucionaria
en la optimización de los \ssolvers secuenciales aporta alguna mejora en este
nuevo escenario. Este segundo objetivo encuentra su motivación en la hipótesis
de que al distribuir el trabajo entre distintas unidades de cómputo se abre la
posibilidad de que se produzca una enorme cantidad de retrabajo que podría ser
sustancialmente minimizado mediante la incorporación de cláusulas aprendidas
por otras instancias. Al mismo tiempo, la comunicación de cláusulas aprendidas
entre distintas unidades de procesamiento conlleva un \emph{overhead}
intrínseco que podría anular los efectos beneficiosos producidos por la
disminución del retrabajo. Esto último, sumado al hecho de que el incremento
en los tamaños de las bases de datos de cláusulas aprendidas de cada instancia
ralentiza la propagación de implicaciones nos pone en una situación en la que
es importante encontrar un equilibrio entre la ganancia obtenida por una parte
y la pérdida producida por el \emph{overhead}. A su vez, la incorporación de
aprendizaje en un ambiente distribuido presenta desafíos propios del entorno
como ser: lidiar con la velocidad de acceso a recursos que se encuentran en
equipos remotos (órdenes de magnitud más lento que el acceso a memoria
principal), la necesidad de tener en cuenta la contención para evitar la
saturación de un equipo, etc. Por lo tanto esta incorporación se diseñará e
implementará sin perjuicio de las características mencionadas anteriormente
que creemos centrales en el desarrollo de una herramienta que pretende
ejecutar en ambientes paralelos y distribuidos que \emph{a priori} no son
conocidos.\\

La presente tesis se divide de la siguiente forma. El Cap.~\ref{preliminares}
presenta las definiciones y resultados básicos del área de \sat, en conjunto
con algunas valoraciones del autor, con el objeto de fijar notación y
establecer un piso de contenidos que permitan el desarrollo de los aspectos
novedosos contenidos en esta tesis. El Cap.~\ref{ssolver-pardist}
presentaremos la arquitectura y los aspectos relevantes de la implementación
de un \ssolver paralelo y distribuido basado en la técnica \gp. El Cap.~\ref
{aprendizaje-pardist} exhibe el diseño e implementación de un mecanismo de
aprendizaje para el \ssolver presentado en el Cap.~\ref{ssolver-pardist} en
conjunto con una prueba de concepto de la técnica, el desarrollo de los
experimentos y la sistematización de resultados. Para finalizar, en el
Cap.~\ref{conclu}, presentaremos las conclusiones derivadas del desarrollo de
la herramienta y los experimentos sobre ella realizados, y las líneas de
investigación que se abren a partir de esto.
