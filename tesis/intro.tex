%!TEX root = tesis.tex

En la actualidad el \soft forma parte integral de nuestra vida cotidiana. El
mismo interviene en todo tipo de tareas. Desde el envío de un mensaje de texto
o la realización de una llamada telefónica hasta el sistema de control de una
central nuclear, pasando por el control de un ascensor o el sistema de
velocidad crucero de un automóvil, utilizan sistemas basados en \hard y \soft
informático. La variedad, alcance y \textbf{criticidad} de las
responsabilidades asignadas a las piezas de \soft hacen de las mismas un
componente de alto impacto en nuestra realidad actual.

Por otra parte, la alta complejidad del \soft hace que la construcción del
mismo sea una tarea propensa a errores. El impacto de una falla en un
componente de \soft puede variar desde la imposibilidad de utilizar un
artefacto doméstico hasta una catástofre de magnitudes como la fundición del
núcleo de un reactor nuclear.

Son estos aspectos los que motivan la necesidad creciente de construir métodos
(formalismos, metodologías, herramientas, etc.) que permitan garantizar la
calidad del \soft en un sentido general. La posibilidad de establecer la
ausencia de errores en un programa radica en la capacidad de establecer
inequívocamente que ese programa cumple una determinada propiedad que expresa
el adecuamiento del mismo al comportamiento esperado. Por ejemplo si
quisiéramos establecer el correcto  funcionamiento del mecanismo de seguridad
de un horno de microondas podríamos enunciar una propiedad como la siguiente:
``No es posible encender el microondas si la puerta está abierta''.

La \textbf{verificación formal} de \soft consiste en demostrar que un
determinado sistema o algoritmo es correcto respecto a una
\textbf{especificación formal} del mismo. Los lenguajes naturales son
intrínsecamente ambiguos y por lo tanto una descripción de un sistema
realizada en lenguaje natural no puede ser verificada. Una especificación
formal es una descripción en un lenguaje formal de un sistema de \soft. Un
lenguaje para ser formal debe poseer: \begin{inparaenum}[a)] \item reglas para
determinar cuándo una sentencia pertenece al lenguaje (sintaxis) \item reglas
que permitan interpretar las sentencias (bien formadas) de manera precisa y
significativa (semántica) y \item reglas para inferir información útil a
partir de la especificación (\emph{proof theory}) \end{inparaenum}

El enfoque de la verificación formal de software, tal vez uno de los más
ambiciosos en el campo de la ingeniería de software, enfrenta dos problemas
fundamentales. En primer lugar está la dificultad de construir una
especificación formal que exprese de manera correcta el comportamiento
esperado del sistema (validación) ya que el mismo normalmente proviene de
requerimientos expresados en lenguaje natural. El segundo problema fundamental
es que las lógicas subyacentes a la mayoría de los lenguajes formales capaces
de expresar un conjunto lo suficientemente amplio e interesante de propiedades
son normalmente indecidibles, es decir que no existe un algoritmo capaz de,
para toda fórmula del lenguaje, establecer si la misma es verdadera, falsa o
no es un teorema.

El problema de la indecidibilidad se transforma en un escollo fundamental en
tanto impide la construcción de métodos de verificación formal completamente
automáticos. Diversos enfoques han sido desarrollados para atacar este
problema \todo[inline]{Buscar citas. SMT Solving, Bounded MC, etc}. La mayoría
de estos métodos consisten en restringir de alguna manera el lenguaje formal
utilizado de modo de generar un sublenguaje del mismo que sea decidible. Uno
de los enfoques desarrollados en este sentido es el conocido como \bmc
(verficación acotada de modelos).

\mc\cite{emerson:scp-2_3} es una técnica desarrollada para la verificación de
sistemas reactivos. La técnica consiste en la exploración exhaustiva y
automática del espacio de estados de un sistema modelado como una máquina de
estados finita. Cuando la cantidad de estados del sistema es relativamente
grande la exploración exhaustiva del espacio de estados se vuelve inpractible.
Esto motivó el desarrollo de la técnica conocida como \smc\cite{burch:lics90,
mcmillan93} que ataca el problema antedicho a partir de evitar construir el
grafo de estados del sistema utilizando fórmulas proposicionales para
caracterizar conjuntos y relaciones. En \smc el chequeo de las fórmulas
proposicionales es realizada típicamente mediante la manipulación de BDDs
(\bdds), una forma canónica de representar dichas fórmulas. A pesar de que las
técnicas de \smc basadas en  BDDs son capaces de manejar sistemas con cientos
de estados, el tamaño de los BDDs generados para sistemas grandes se vuelve
prohibitivo.

El problema del espacio requerido por los BDDs motivó la creación de la
técnica conocida como \bmc\cite{Biere:1999:SMC:646483.691738} que reemplaza la
utilización de BDDs por la verificación de fórmulas proposicionales a través
de procedimientos SAT capaces de manejar en la actualidad fórmulas
proposicionales con varios miles de variables. En particular, la técnica
presentada en \cite{Biere:1999:SMC:646483.691738} consiste en buscar
contraejemplos de una longitud acotada $k$ a partir de generar una fórmula
proposicional que sea verdadera si y sólo si un contraejemplo de dicha
longitud existe. Si bien esta técnica es completa para sistemas modelados como
máquinas de estados finitos ya que el espacio de búsqueda, aunque
exponencialmente grande, es finito, no es cierto que esta metodología aplicada
a un lenguaje formal general sea completa\todo{Revisar esto}. La perdida de
completitud en el caso general generada por la introducción de la cota en la
longitud del contraejemplo es lo que, aplicado a un sistema lógico
indecidible, permite obtener un subconjunto del lenguaje original que ahora es
decidible y, por lo tanto, automáticamente verificable \todo{Revisar esto}. Un
ejemplo típico de esto se da en lenguajes donde la indecidibilidad se da a
partir de la existencia de cuantificadores existenciales sobre dominios no
acotados. En estos casos la cota en la longitud del contraejemplo se traduce
en una cota para cada uno de los dominios no acotados. De este modo las
cuantificaciones existenciales se convierten en disjunciones acotadas y por lo
tanto el lenguaje se vuelve decidible (y en particular codificable como una
fórmula proposicional suceptible de ser verificada automáticamente). Por
ejemplo la fórmula \begin{equation}(\exists x)\ (\alpha(x))\end{equation} a
partir de la introducción de la cota sobre el dominio de $x$ se transforma en
la fórmula finita \begin{equation}\left(\exists t \in
\{t_1,\ldots,t_n\}\right)\ \left(\alpha(t)\right)\end{equation} o lo que es lo
mismo \begin{equation} \alpha(t_1) \vee \ldots \vee \alpha(t_n)\end{equation}

Debido a la incorrectitud generada por la introducción de la cota en la
longitud del contraejemplo, \bmc no puede ser considerada una técnica de
verificación en el sentido estricto. Sin embargo la utilización de \bmc
permite ganar confianza en la propiedad que se quiere verificar antes de
embarcarse en la costosa tarea de demostrarla formalmente. Además \bmc ha sido
aplicada con éxito a otras áreas relacionadas como la generación automática de
casos de test, validación de código y especificaciones, etc.


\section{Alloy}

Existen una variedad de herramientas formales que utilizan el enfoque de \bmc
aplicado a la especificación y verificación de \soft. Alloy \cite{jackson
:acmtosem-11_2} es un lenguaje formal diseñado para expresar las propiedades
estructurales de un sistema. Posee una sintaxis declarativa lo suficientemente
poderosa como para expresar propiedades complejas y a la vez plausible de ser
analizada de forma completamente automática. De hecho, Alloy es también una
herramienta capaz de analizar automáticamente especificaciones escritas en
lenguaje Alloy.

\missingfigure{Ejemplo especificación Alloy}

\todo{Sarabastaza de Alloy hasta llegar a que se traduce a un problema SAT.
Tal vez mencionar extensiones como DynAlloy y casos de estudio exitosos}

\section{SAT Solving}

El interés en construir procedimientos sistemáticos y automáticos de
demostración se remonta al interés que tuvieron diversos matemáticos en
construir un método de esas características para la lógica de primer orden.
Uno de los máximos exponentes fue David Hilbert, quien enunció que la búsqueda
de un algoritmo de decisión para la lógica de primer orden era el problema
central de la lógica matemática. La demostración de que un procedimiento de
estas características no era posible elaborada por Church y Turing provocó una
pérdida de interés en esta clase de problemas. Sin embargo la construcción de
procedimientos, no ya de decisión, sino de demostración para la lógica de
primer orden (entre los cuales se destaca el desarrollado por Martin Davis y
Hilary Putnam \cite{Davis:1960:CPQ:321033.321034}) sumada a los crecientes
campos de aplicación, dentro de las ciencias de la computación, de lógicas
decidibles o de fragmentos decidibles de lógicas no decidibles, dieron un
nuevo impulso al desarrollo de estos métodos.

En particular la variedad de aplicaciones que fueron surgiendo para la lógica
proposicional (\emph{Planning} en inteligencia artificial, demostración
automática de teoremas, \textbf{\mc}, \textbf{verificación de \soft}, etc.)
impulsaron fuertemente el desarrollo de técnicas y procedimientos para
determinar si una fórmula de la lógica proposicional podía ser verdadera. El
establecimiento de este problema como el primer problema \npc
\cite{Cook:1971:CTP:800157.805047} no impidió el desarrollo de heurísticas que
hoy permiten manejar fórmulas de cientos de miles de variables eficientemente.

Si bien existe una variedad de métodos para determinar si una fórmula
proposicional puede ser verdadera, los más utilizados son aquellos derivados
del algoritmo desarrollado por Davis, Longemann y Loveland (\dpll) en
\cite{Davis:1962:MPT:368273.368557} que es un refinamiento del algoritmo
presentado en \cite{Davis:1960:CPQ:321033.321034}. Este procedimiento es, en
esencia, un algoritmo de \bt.

Tuvieron que pasar varios años para que Marques-Silva introdujera en \cite
{marques-silva:iccad96} la primera optimización radical al algoritmo \dpll que
dio origen a lo que hoy se conoce como procedimiento \cdcl (\CDCL) cuya base
es el aprendizaje de nuevas cláusulas ante cada conflicto. Ésto permite
recortar el espacio de búsqueda de manerea dramática. A partir de ese momento
una serie de optimizaciones han sido desarrolladas para cada una de las partes
del algoritmo desarrollado por Marques-Silva. La mayoría de estas
optimizaciones \todo[inline]{¿Todas?} son heurísticas para cada uno de los
puntos de decisión del algoritmo \footnote{Para un relevamiento completo de
las optimizaciones aplicadas al algoritmo \CDCL ver \cite{manthey:mathesis}}.

A pesar de la mejora que representó la incorporación de aprendizaje propuesta
por Marques-Silva, con el correr del tiempo se estableció que los \ssolvers
invierten la mayor parte de su tiempo (cerca del $90\%$) en propagar las
implicaciones que se siguen de cada una de las decisiones tomadas durante la
búsqueda. Esta propagación incluye el recorrido líneal \todo[inline]{Verificar
``lineal''} del conjunto de cláusulas que conforman el problema. Por lo tanto,
la incorporación de nuevas cláusulas (el \emph{leitmotiv} del procedimiento
\CDCL) a la vez que proporciona una ventaja al recortar el espacio de búsqueda
representa una desventaja en tanto que incrementa la cantidad de cláusulas
sobre las cuales es necesario realizar las propagaciones. Esta situación,
sumado al hecho de que la incorporación de nuevas cláusulas también representa
un problema desde el punto de vista de la utilización de la memoria de la
computadora, han llevado a la comunidada a establecer que, por lo general, es
mejor tener un política agresiva de recorte de la base de datos de cláusulas
aprendidas \cite{Audemard:2009:PLC:1661445.1661509}. Si bien se han
desarrollado distintas heurísticas con muy buenos resultados, el problema de
determinar cuántas y cuáles cláusulas descartar (o su equivalente, preservar)
sigue siendo un problema ampliamente estudiado en el mundo \sat.

\section{SAT Solving paralelo y distribuido}

El establecimiento del problema \sat como un problema \npc hace pensar que no
es posible obtener un algoritmo polinomial que resuelva dicho problema. Además
en los últimos años la rapidez en el incremento de la velocidad de las
computadoras ha ido disminuyendo (esencialmente debido al problema de la
disipación del calor y el consumo de energía) y en su lugar se ha producido un
incremento en la cantidad de unidades de procesamiento disponibles así como su
abaratamiento. Estos hechos han impulsado a la comunidad \sat a explorar la
posibilidad de construir algortimos paralelos y distribuidos de \ssolving de
modo de \begin{inparaenum}[a)] \item sacar el máximo provecho de la nueva
tendencia en construcción de computadoras y \item proveer un algoritmo
\textbf{escalable} que permita aprovechar el abaratamiento del \hard
\end{inparaenum}.

La aparición de los primeros \ssolvers paralelos y distribuidos
\cite{bohm:1996:afast, zhang:jsc-1996} es contemporánea a la aparición del
algoritmo \CDCL. Por lo tanto los primeros \ssolvers paralelos fueron
desarrollados sobre la base del algoritmo \dpll. Es decir que estos \ssolvers
no incorporaban aprendizaje de nuevas cláusulas.

Una de las razones que llevaron a que existieran implementaciones tempranas de
algoritmos paralelos para el problema \sat es que este problema presenta una
forma intuitiva y fácil de ser dividido. La idea consiste en seleccionar un
subconjunto pequeño de variables del problema y generar las $2^n$ asignaciones
posibles de valores de verdad a este subconjunto. Cada una de estas
asignaciones es una asignación parcial del problema original y por lo tanto lo
que obtenemos son $2^n$ subproblemas \textbf{disjuntos} que pueden ser
enviados a distintas unidades de procesamiento. Cada unidad de procesamiento
puede utilizar entonces alguna implementación de un algoritmo de \ssolving
secuencial para resolver el subproblema que le haya tocado. Esta técnica,
llamada \gp, introduce una pregunta fundamental: ¿De qué modo determinar un
\gp efectivo? Esto es, cómo construir un \gp que distribuya equitativamente la
carga de trabajo entre las distintas unidades de procesamiento.

El enfoque anterior de división de un problema \sat es adecuado para un
escenario con unidades de procesamiento que se comunican a través de una red,
como pude ser un \emph{cluster} o en computación \emph{grid}. La proliferación
de computadoras con múltiples procesadores y múltiples núcleos impulsó la
investigación sobre formas de paralelización de \ssolvers en un escenario
donde las múltiples unidades de procesamiento tienen acceso uniforme a una
memoria común. Este nuevo escenario permitió el desarrollo de una forma
radicalmente distinta de paralelización de un problema \sat. La misma consiste
escencialmente en lo que se conoce como \emph{portfolio}. La idea se
fundamenta en la observación de que distintos \ssolvers secuenciales, o un
mismo \ssolver con distintos parámetros, pueden resolver un mismo problema en
tiempos sumamente diferentes. Sobre esta base se desarrollaron \ssolvers
paralelos que en lugar de dividir el problema a resolver, ejecutan distintos
\ssolvers sobre el mismo problema (o un mismo \ssolver con distintos
parámetros) y comparten las cláusulas aprendidas entre las distintas
instancias. Por supuesto, distintas implementaciones concretas han sido
desarrolladas sobre esta idea, e incluso implementaciones híbridas que
combinan las dos formas de paralelización antedichas\todo{Faltan citas}. 
