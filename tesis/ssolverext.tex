%!TEX root = tesis.tex

\section{Sobre la automatización}

En la presente sección nos proponemos señalar los aspectos importantes a tener
en cuenta relacionados con la automatización del funcionamiento de nuestra
herramienta. Además señalaremos las decisiones adoptadas por nosotros en cada
uno de los puntos relevantes.

\subsection{Consideraciones sobre la estrategia}
% - Noción de problema difícil vs problema fácil
% - Imposibilidad de determinar o siquiera estimar eso a priori
% - Enorme varianza entre los subproblemas de un problema

% - Cuándo declarar que un problema es "demasiado difícil" (para
% atacarlo sin partirlo)?
% - Cuáles de los solvings en curso abortar? Cuántos? Cuándo?

%         - Imposibilidad de determinar o siquiera estimar a priori (en base a
%                 criterios estáticos) la dificultad de un (sub)problema.

%         - No sabemos cuánto va a tardar.
%         - No sabemos si es fácil o difícil.

%         - Partir temprano algo fácil (una rama que con poco más se cerraba)
%           es un error que se paga caro, sobre todo si se repite recursivamente.

%         - Invertir mucho esfuerzo en algo difícil y encima no lograr cerrarlo
%           (sólo para terminar partiéndolo igual, y mucho después) es otro
%           error que se paga caro, sobre todo si se repite recursivamente.

%         - Invertir mucho esfuerzo en algo difícil hasta cerrarlo sin partirlo
%           no siempre es una buena idea (atenta contra el paralelismo,
%           sube el camino crítico, etc).

%         - No existe un valor de TO prefijado que sea adecuado para cualquier
%           problema raíz (y no queremos que el usuario tenga que proveer uno).

%         - Incluso para cierto problema raíz R, no existe un único valor de TO
%           que sea adecuado durante todo el transcurso de la corrida.

%                 - Cada vez que se parte un subproblema (sup. elección razonable de
%                   vars) se obtienen hijos más fáciles, pero

%                     - con mucha varianza en su distribución [tablita pamela8?]

%                     - con tan poca predecibilidad como dijimos antes respecto de
%                       la tasa (hijo_mas_caro/padre)

%                 - O sea que lo único que realmente sabemos es que "se van haciendo
%                   más fáciles" (pero no a qué velocidad, ni cuánto más rápido a lo
%                   largo de una rama vs. de otra, etc).

%         => Es necesario algún criterio dinámico / adaptativo / etc.

%         - Podría basarse en
%                 - aprendizaje sobre el problema (qué pasó hasta ahora otras veces, etc)
%                 - feedback loop observando métricas del comportamiento/estado del sistema


% - Cómo partir un problema demasiado difícil?

%         - Cuáles y cuántas variables levantar

%                 => Gran Temón en sat-solving, recontra determinante para la
%                   cant/dificultad/distribución de los subproblemas, etc
%                   VSIDS, etc.

%                 => Nivel de agresividad, fanout, potencial explosión;
%                         trade-off "muchos" vs "más fáciles" (ojalá!);
%                         hay que tener cuidado con el fanout recursivo.

%         - Filtrado de subproblemas triviales

%                 - Rara vez se da el peor caso 2^n totalmente denso
%                 - Muchas veces se puede lograr mucho menos!
%                 - Hay maneras muy baratas de filtrar los muy triviales
%                 - Hay maneras (no tan) baratas de filtrar los (no tan) triviales
%                 => Trade-off:
%                         - escaso filtrado =>
%                             overhead innecesario por proliferación de tareas fáciles evitables
%                         - exceso de esfuerzo en filtrado =>
%                             excesiva centralización de costo computacional que podría distribuirse


% - Ni bien alguien queda ocioso debe asignársele más trabajo.

% - Decisión importante: ¿cuál de todas las tareas pendientes es la "próxima"?
%   => afecta cómo se recorre el espacio de búsqueda
%   => termina afectando el scheduling (en presencia de feedback loops etc)

% - Además, minimizar costos de movimientos de tareas innecesarios
%   => si ese alguien ya tiene trabajo pendiente local, podría ser bueno
%      que se le asigne lo mejor posible dentro de lo local

Comenzaremos esta sección desglosando las principales cuestiones a
resolver por una estrategia completamente automática para nuestra herramienta.
Surge así la primer pregunta relevante que cualquier estrategia debe
responder: ¿Cuándo declarar que un (sub)problema es demasiado difícil para
atacarlo como tal? Es decir ¿Cuándo se toma la decisión de partir un problema
en nuevos subproblemas?

\subsubsection{Dificultad de un problema}

En este punto es importante destacar el hecho de que la partición de un
problema en nuevos subproblemas se lleva a cabo con el objetivo de distribuir
las distintas porciones del espacio de búsqueda entre distintas unidades de
cómputo. Así, invertir demasiado tiempo en intentar obtener un resultado
secuencialmente para un problema difícil atenta prohibitivamente contra el
paralelismo. Por lo tanto la decisión de cuándo abortar un \solving en curso
se vuelve crucial.

Si un problema va a tomar demasiado tiempo no tiene sentido invertir tiempo de
\solving secuencial en intentar resolverlo y es conveniente partirlo
\emph{cuanto antes}. Por otra parte, partir un problema en un momento tal que
si hubiéramos esperado una fracción  pequeña de tiempo más el \w habría
arribado a un resultado para el problema implica que \emph{todo} el tiempo de
cómputo previamente invertido es desperdiciado. Estos dos errores se vuelven
catastróficos cuando se repiten en forma sistemática sobre los subproblemas a los que un problema da origen.

Subyace a toda esta cuestión la pregunta fundamental de cómo determinar que un
problema dado es fácil o difícil. Si tuviéramos algún criterio que nos
permitiese siquiera aproximar la dificultad de un problema dado, o el
porcentaje de avance durante una corrida secuencial de un \ssolver, podríamos
elaborar una estrategia que atienda razonablemente a las inquietudes
planteadas arriba. Lamentablemente no se conoce ninguna métrica que permita
establecer \apriori la dificultad de un problema. Si bien la complejidad de
los algoritmos de \ssolving se expresa en función de la cantidad de variables
de un problema (y es exponencial), la experiencia indica que existen problemas
con pocas variables sumamente difíciles y problemas con muchas variables
razonablemente sencillos.

Resulta claro entonces que una primera aproximación \emph{naive} a este
problema como podría ser la elección de un \tout fijo como criterio para
determinar cuándo un problema debe ser partido, resulta una mala alternativa.
La disparidad en la dificultad de los distintos problemas hace que sea
imposible atacar razonablemente todos los problemas con un único valor de \tout.
Una posibilidad sería entonces que el usuario seleccionara el \tout que quiere
utilizar para cada problema. Sin embargo esto atenta sustancialmente contra la
usabilidad de la herramienta ya que requiere asumir un \emph{expertise} por
parte del usuario y un conocimiento previo del problema a resolver que no son
razonables. 

Además a medida que partimos recursivamente un problema, los subproblemas
generados --si la elección de variables a levantar fue apropiada-- son
\emph{cada vez} más fáciles. Por lo tanto un valor de \tout que resulta
adecuado en un momento no lo será más adelante. Esto podría llevarnos a
elaborar un criterio un tanto más refinado que lo anterior. Por ejemplo,
podríamos optar por tener un valor de \tout para cada \emph{nivel} del
problema. Sin embargo surge nuevamente el mismo problema que mencionábamos
anteriormente. Cada vez que un problema es partido, los subproblemas generados
difieren notablemente en su nivel de dificultad. Más aún, las distribuciones
de dificultad entre los hijos de dos subproblemas distintos de un mismo
problema (a misma cantidad de variables levantadas) pueden ser muy distintas.
Por lo tanto, este refinamiento tampoco resulta adecuado.

En general, la ausencia de un criterio estático para determinar la dificultad
de un problema dado, hace imposible la elección de un criterio de corte que
sea también estático. Por lo tanto se vuelve necesario que el criterio para
determinar que un problema debe ser partido sea dinámico. Es decir, que el
mismo se vaya adaptando durante la ejecución. Un criterio de estas
características tiene dos fuentes de información fundamentales. Por un lado el
estado general del sistema y su evolución (su historia). Esto puede incluir
métricas sobra la carga del sistema, cantidad de problemas pendientes, espacio
de almacenamiento disponible, etc. La segunda fuente de información surge de
las características del problema que sean observables a lo largo de la
ejecución. Ejemplos de esto son la cantidad de subproblemas producidos cada
vez que se partió un problema, estadísticas sobre los tiempos que demoraron
los subproblemas ya resueltos, etc. 

Cualquier estrategia automática razonable deberá entoces dar respuesta a la
problemática de decidir cuándo partir un problema teniendo en cuenta las
observaciones mencionadas. Una vez resuelto este dilema surge la segunda
pregunta fundamental a responder que es ¿Cómo partir un problema? Que
esencialmente se traduce en ¿Cuántas y qué variables levantar en cada ocasión?

\subsubsection{Criterio de particionado}

\newcommand{\vsids}{VSIDS\xspace}

La determinación de qué variables levantar en el momento en que se decide
partir un problema es uno de los grandes temas en el mundo del \ssolving. En
un sentido, este problema es equivalente al problema que enfrentan los
\ssolver secuenciales cuando deben tomar una decisión. Cuando un \ssolver
secuencial agotó las propagaciones de la última decisión tomada, debe tomar
una nueva decisión. Tomar una decisión implica en primera instancia
seleccionar una variable y luego seleccionar cuál de los dos posibles valores
de verdad asignar a esa variable. En nuestro caso, dado que al levantar una
variable los dos valores de verdad son considerados \emph{simultáneamente}, el
problema se reduce a elegir cuidadosamente la variable a levantar. Uno de los
métodos más difundidos para elegir la próxima variable en los \ssolvers
secuenciales, se basa en un criterio de actividad conocido como \vsids. Del
mismo modo que para las cláusulas --ver Sec.~\ref{sec:longactlbd}-- cada vez que
una variable \emph{participa} de un conflicto su actividad es incrementada.
Luego, cuando el \ssolver se encuentra en la situación de tener que elegir una
variable para continuar la búsqueda, elige aquella con mayor actividad.

La elección de las variables a ser levantadas es crucial. Como se ve en
la Tabla~\ref{tablita} una buena elección de variables puede generar una
descendencia muy fácil o muy difícil. En general, no se posible saber cuál es
la mejor elección de variables a ser levantadas. Sin embargo la heurística
\vsids ha reportado muy buenos resultados en el mundo del \ssolving secuencial.


\marginpar{Acomodar la tabla y corregir la referencia que aparece antes.}

\begin{tabular}{|c|c|r|r|r|r|r|}
\hline
Subconjunto & Cantidad de hijos & & & & & \\
de variables & no triviales & M\'aximo & Media & Suma total & Desv.
est. & Mediana \\
\hline
A 	& 136 	& 8.09 		& 1.13 		& 154.17	 	& 1.37 		& 0.51 \\
B 	& 136 	& 18.06 		& 1.28 		& 173.48	 	& 1.95 		& 0.72 \\
C 	& 192 	& 36.25 		& 8.74 		& 1678.88 	& 7.45 		& 5.78 \\
D	& 174 	& 63.62 		& 1.25 		& 218.29	 	& 6.27 		& 0.05 \\
E	& 192 	& 89.41 		& 2.18 		& 418.04	 	& 7.66 		& 0.39 \\
F 	& 192 	& 288.79	 	& 10.18 		& 1954.07 	& 22.36 		& 2.46 \\
G 	& 256 	& 376.70	 	& 86.03 		& 22024.53 	& 81.98 		& 56.98 \\
\hline
\end{tabular}

Además de la determinación de qué variables levantar, es importante también
tener en cuenta cuántas levantar en cada momento. En primer lugar, es
importante tener en cuenta que la cantidad potencial de subproblemas generados
es exponencial con respecto a las variables levantadas. Esto afecta en un
doble sentido. En primera instancia es necesario observar que, dado que el
proceso de particionado se repite recursivamente, la generación de una
excesiva cantidad de subproblemas ante cada particionado genera una explosión
exponencial de problemas. Si este factor no es tenido en cuenta es altamente
probable que el sistema diverja y no sea capaz de arribar a una solución.
Sumado a esto, el tiempo insumido en generar los subproblemas producto de un
problema puede volverse excesivamente grande atentando una vez más contra el
paralelismo y la eficiencia de la herramienta.

Una vez más debemos recordar que la idea de levantar variables se basa en que
--suponiendo una elección razonable de cuáles levantar-- cuanto mayor sea la
cantidad levantada, más fáciles serán los subproblemas generados. Es por
esto que si la cantidad de variables a levantar es demasiado pequeña, es
probable que los subproblemas que sean generados no sean suficientemente 
fáciles como para ser resueltos y que, por lo tanto, sea necesario partirlos también en el
futuro haciendo que el tiempo invertido en intentar resolverlo se convierta en
tiempo desperdiciado. Además, si la cantidad de variables levantadas es muy
baja, existe la posibilidad de que la cantidad de subproblemas generados no
sea suficiente para aprovechar al máximo el \hard disponible.

La última observación referida al criterio de particionado tiene que ver con
qué problemas son dignos de ser distribuidos realmente. Hemos mencionado
muchas veces que al levantar $n$ variables, la cantidad \textbf{potencial} de
subproblemas generados es $2^n$. El uso de la palabra potencial tiene que ver
con el hecho de que varios de los subproblemas generados al levantar $n$
variables podrían ser resueltos trivialmente. ¿A qué nos referimos con
trivialmente? En primer lugar a aquellos subproblemas en los que alcanza con
propagar\footnote{El uso del término propagación en este contexto es estricto.
Es decir que nos referimos exclusivamente a la realización de la clausura de
\bcp sobre el subproblema. Esta propagación no incluye la toma de nuevas
decisiones.} las implicaciones de las decisiones tomadas para obtener un
resultado --\sat o \unsat--. Pero más en general, nos referimos a todos
aquellos subproblemas en los que el \emph{overhead} de generar la tarea,
trasladarla a otro \w, cargar la tarea en el nuevo \w, y \solvear dicho
problema hasta arribar a un resultado sea suficientemente grande como para que
sea conveniente resolverlo directamente sin generar
una nueva tarea.

Se introduce entonces una nueva variable en el criterio de particionado que es
cuáles de los potenciales $2^n$ subproblemas generar como tareas a ser
resueltas por otros \ws y cuáles resolver directamente sin generarlos como nuevas
tareas. Se debe observar que \emph{no generar} un subproblema requiere invertir
un tiempo de cómputo suficiente para determinar el resultado de ese
subproblema. Una estrategia automática debe entonces decidir qué hacer con
esta variable teniendo en cuenta que invertir poco o nada de tiempo de cómputo
en filtrar los subproblemas puede degradar significativamente la eficiencia
del sistema debido a que una cantidad de subproblemas
demasiado fáciles serán generados como nuevas tareas. Pero también debe tener en cuenta que invertir demasiado
tiempo de cómputo en el filtrado de los subproblemas tiende a degradar
significativamente el paralelismo.

\subsubsection{Otros aspectos de interés}

Otro aspecto que influye en la \emph{performance} de la herramienta es la
forma en que se recorre el espacio de búsqueda. Cuando un \w queda ocioso debe
tomar una nueva tarea (si hubiera tareas pendientes). Surge entonces el
problema de determinar cuál de todas las tareas pendientes debe ser atacada.
Como en los casos anteriores existen dos factores que deben ser balanceados.
Por un lado, si la estrategia es adaptable mediante métricas dinámicas, el
orden en el que se recorren las tareas a resolver alterará el resultado.
\todo{Ver si hacerlo más detallado esto} Por lo tanto nos gustaría poder
seguir el orden \emph{más adecuado} a las métricas que hayamos utilizado en
nuestra estrategia. Sin embargo, la tarea \emph{ideal} puede encontrarse
almacenada en otro \w. Surge así el segundo factor a tener en cuenta. En aras
de la escalabilidad, el criterio para elegir la próxima tarea a ser atacada no
debe generar una sobreutilización de la red de comunicaciones.

Un último factor a tener en cuenta es la necesidad de maximizar la utilización
del \hard disponible. Es decir que, idealmente, si hay \hard ocioso no debería
haber tareas pendientes.

\subsection{La estrategia implementada}

Teniendo en cuenta los factores mencionados anteriormente nos volcamos al
desarrollo de una estrategia automática. Nuestro objetivo no estuvo puesto en
el desarrollo de una estrategia óptima ya que cada uno de los factores
mencionados con anterioridad podría ser objeto de un extenso estudio, y aún
así no existen garantías de que existe tal cosa como la estrategia óptima para
toda clase de problemas. Por lo tanto nuestro esfuerzo estuvo dedicado a no
incurrir en los errores más flagrantes, de modo que la herramienta funcionase
razonablemente bien con una serie de problemas y poder empezar a obtener
resultados que nos permitan resaltar los aspectos fuertes y los aspectos que
aún necesitan un esfuerzo importante.

En cuanto al criterio de particionado las decisiones tomadas fueron:

\begin{itemize}
	\item La cantidad de variables a levantar se fijó en 10. Aunque este valor puede ser alterado por el usuario.
	\item Las variables a levantar se seleccionan utilizando la heurística \vsids. Es decir que cuando un problema va a ser partido, se consultan las actividades de cada variable y se seleccionan --de acuerdo con el punto anterior-- las 10 variables más activas.
	\item Cada potencial subproblema es \solveado por 50ms. Si en ese tiempo no se arriba a un resultado, el problema pasa a ser una nueva tarea pendiente.
\end{itemize}

Respecto al orden en el cual se recorre el espacio de búsqueda:

\begin{itemize}
	\item Si un \w queda ocioso y no tiene tareas pendientes en su espacio de almacenamiento, obtiene la tarea que corresponda de acuerdo a la estrategia BFS --\emph{breadth-first search}--.
	\item Si al quedar ocioso el \w cuenta con tareas pendientes locales, se selecciona la que corresponda por BFS de las tareas locales.
\end{itemize}

Sobre cuándo declarar que un problema es demasiado difícil, adoptamos una
estrategia basada en el estado del sistema. En concreto nos enfocamos en que
el sistema mantenga una cierta tasa de progreso. Para ello medimos la cantidad
de subproblemas resueltos por unidad de tiempo y luego establecimos un umbral
objetivo. Luego, si la frecuencia actual se encuentra por debajo del umbral
objetivo, entonces debemos tomar uno de los problemas que actualmente están
siendo atacados y partirlo. Llamaremos \textbf{frecuencia objetivo} al umbral
de problemas terminados por segundo por debajo del cual se decide que algún
problema debe ser partido. Llamaremos \textbf{frecuencia actual} a un valor
que estima la cantidad de problemas terminados por segundo a partir de
promediar los problemas que han terminado durante los últimos $t$ segundos.
Llamaremos \textbf{tamaño de la ventana de muestreo} al mencionado valor $t$.
Llamaremos \textbf{frecuencia de checkeo} a la frecuencia con la que se revisa
si la frecuencia actual se encuentra por encima de la frecuencia objetivo.

En concreto los parámetros utilizados fueron:

\begin{itemize}
	\item Frecuencia objetivo: $0.15$ p/\w
	\item Tamaño de la ventana de muestreo: $20s$
	\item Frecuencia de checkeo: $5s$
\end{itemize}

Que la frecuencia objetivo dependa de la cantidad de \ws permite ajustar la
cantidad de problemas generados de acuerdo a la capacidad de consumirlos que
tiene el sistema. Es importante notar que el tamaño de la venta influye en
cuánto peso se le da a la historia. Una ventana más grande es más estable
mientras que una ventana más chica se adapta más rápidamente ya que presenta
mayor volatilidad. La frecuencia de checkeo administra cuánto tiempo de
tolerancia se tiene ante una frecuencia actual menor a una frecuencia
objetivo. Si la frecuencia de checkeo es alta --menor valor-- entonces la
tolerancia será baja y ante cualquier caída de la frecuencia actual por debajo
del objetivo el sistema reaccionará. Por otro lado una frecuancia alta --mayor
valor-- representará una tolerancia mayor dando oportunidad al sistema de
tener períodos en los que la frecuencia actual no alcanza la frecuencia
objetivo. La combinación de  estos tres parámetros configura la agresividad
del sistema a la hora de determinar que un problema es demasiado difícil.

Además del criterio por frecuencia decidimos que ante la situación en que hay
\ws sin tareas para realizar y en el sistema no hay más tareas pendientes se
debe partir un problema en curso. De este modo evitamos que haya recursos
ociosos. En particular esta decisión es útil al comienzo de la ejecución de un
problema para distribuir rápidamente el trabajo entre las distintas unidades
de cómputo.

Falta determinar entonces cuál de los problemas en curso será el seleccionado
para ser partido. En este punto optamos por que sea aquel en el que se ha
invertido más tiempo de cómputo. De este modo evitamos que un \w quede atorado
intentando resolver un problema que podría ser extremadamente difícil a la vez
que minimizamos las veces en que un problema es partido demasiado pronto.

Con respecto al balanceo del almacenamiento se tomó la decisión de que cada
vez que un \w se queda ocioso y sin tareas pendientes, además de obtener la
tarea que corresponde por BFS, el \w obtendrá una conjunto de tareas pendietes
de aquel \w que posea mayor cantidad. Llamemos $w_{idle}$ al \w que quedón sin
tareas y $w_{overwhelmed}$ al que más tareas pendientes posee, entonces la
cantidad de tareas transferidas en esta ocasión será \begin{equation}
\frac{\#tareas\_pendientes(w_{overwhelmed})}{\# workers}
\label{eq:share}\end{equation} La idea detrás de esto es que $w_{idle}$
obtiene \emph{la porción que le corresponde} de las tareas de
$w_{overwhelmed}$. Para evitar transferencias extremadamente largas, la
cantidad expresada en Eq.-\ref{eq:share} satura en $10$ tareas. Para evitar
una mala distribución\todo{¿Vale la pena entrar en detalle sobre qué significa
mala distribución?}, las tareas que conforman una porción a ser transferida
son seleccionadas al azar del conjunto de tareas pendientes de
$w_{overwhelmed}$.

\subsection{Decisiones que vale la pena seguir investigando}

\section{Resultados experimentales}

\hspace{-5em}
\begin{minipage}{\textwidth}
	\small
	\begin{tabular}{lrrrrrr}
		\toprule
		problem	&	scope	&	seq. runtime (s)	&	par. walltime (s)	&	speedup	&	efficiency \\
		\cmidrule(r){1-6}
		Pamela	&	8	&	308.26	&	60.46	&	5.10x	&	0.08 \\
		Pamela	&	9	&	76168.16	&	407.34	&	186.99x	&	2.92 \\
		Pamela\footnote{Se ejecutó por 14 días sin que el \ssolver secuencial consiga un resultado}	&	10	&	$>$1209600.00	&	5046.84	&	$>$239.67x	&	$>$3.74 \\
		\cmidrule(r){1-6}
		Closure	&	11	&	749.65	&	291.28	&	2.57x	&	0.04 \\
		Closure	&	12	&	3983.36	&	1914.45	&	2.08x	&	0.03 \\
		Closure	&	13	&	16261.35	&	4362.61	&	3.73x	&	0.06 \\
		\cmidrule(r){1-6}
		MarkGC Soundness2	&	9	&	217.31	&	200.85	&	1.08x	&	0.02 \\
		MarkGC Soundness2	&	10	&	2855.30	&	1376.89	&	2.07x	&	0.03 \\
		\cmidrule(r){1-6}
		MarkGC Completeness	&	9	&	18643.06	&	1825.60	&	10.21x	&	0.16 \\
		\bottomrule
	\end{tabular}
	%\caption{Tiempo de ejecución (en segundos) distribuido vs. secuencial}
\end{minipage}
